name: Daily Database Backup

on:
  schedule:
    # Run at midnight UTC every day (00:00)
    - cron: '0 0 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Set timestamp
        id: timestamp
        run: echo "timestamp=$(date -u +'%Y-%m-%d-%H%M%S')" >> $GITHUB_OUTPUT

      - name: Install AWS CLI
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.GH_AWS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.GH_AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.GH_AWS_REGION }}

      - name: Backup PostgreSQL Database and Upload to S3
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.TESTNET_DEPLOY_HOST }}
          username: ${{ secrets.TESTNET_DEPLOY_USER }}
          key: ${{ secrets.TESTNET_DEPLOY_KEY }}
          script: |
            # Create timestamp for the backup filename
            TIMESTAMP=$(date -u +'%Y-%m-%d-%H%M%S')
            BACKUP_FILENAME="postgres-backup-${TIMESTAMP}.sql.gz"
            TEMP_DIR=$(mktemp -d)
            
            # Change to application directory
            cd /opt/megadata-api
            
            # Create backup using docker compose
            echo "Creating database backup..."
            docker compose -f docker-compose.prod.yml exec -T postgres pg_dump -U postgres postgres | gzip > $TEMP_DIR/$BACKUP_FILENAME
            
            # Install AWS CLI if not already installed
            if ! command -v aws &> /dev/null; then
              echo "Installing AWS CLI..."
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "$TEMP_DIR/awscliv2.zip"
              unzip -q "$TEMP_DIR/awscliv2.zip" -d "$TEMP_DIR"
              sudo $TEMP_DIR/aws/install
            fi
            
            # Configure AWS credentials temporarily
            export AWS_ACCESS_KEY_ID="${{ secrets.GH_AWS_ACCESS_KEY }}"
            export AWS_SECRET_ACCESS_KEY="${{ secrets.GH_AWS_SECRET_ACCESS_KEY }}"
            export AWS_REGION="${{ secrets.GH_AWS_REGION }}"
            
            # Upload the backup to S3
            echo "Uploading backup to S3..."
            aws s3 cp "$TEMP_DIR/$BACKUP_FILENAME" "s3://${{ secrets.GH_AWS_BUCKET_NAME }}/backups/$BACKUP_FILENAME"
            
            # Clean up the local backup file
            rm -rf "$TEMP_DIR"
            
            echo "Backup uploaded to s3://${{ secrets.GH_AWS_BUCKET_NAME }}/backups/$BACKUP_FILENAME"

      - name: Cleanup old backups
        run: |
          # List all backup files
          BACKUP_FILES=$(aws s3 ls s3://${{ secrets.GH_AWS_BUCKET_NAME }}/backups/ --recursive | grep postgres-backup | sort)
          
          # Count the number of backup files
          BACKUP_COUNT=$(echo "$BACKUP_FILES" | wc -l)
          
          # If we have more than 14 backups (14 days retention), delete the oldest ones
          if [ "$BACKUP_COUNT" -gt 14 ]; then
            # Calculate how many to delete
            DELETE_COUNT=$((BACKUP_COUNT - 14))
            
            # Get the list of files to delete (the oldest ones)
            FILES_TO_DELETE=$(echo "$BACKUP_FILES" | head -n $DELETE_COUNT | awk '{print $4}')
            
            # Delete each file
            for FILE in $FILES_TO_DELETE; do
              echo "Deleting old backup: $FILE"
              aws s3 rm "s3://${{ secrets.GH_AWS_BUCKET_NAME }}/$FILE"
            done
          else
            echo "Only $BACKUP_COUNT backups found, no cleanup needed"
          fi 